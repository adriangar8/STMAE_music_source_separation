{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSDB18 Dataset Data Augmentation \n",
    "| Name         | Surname    | ID        |\n",
    "|--------------|------------|-----------|\n",
    "| ABOUELAZM    | Youssef    | 10960436  |\n",
    "| BINGLING     | Wu         | 11105141  |\n",
    "| GARCIA       | Adrian     | 10975956  |\n",
    "| OUALI        | Ernest     | 10984484  |\n",
    "\n",
    "This notebook performs data augmentation on MUSDB18 dataset.\n",
    "\n",
    "<b> Features of the script:</b>\n",
    "\n",
    "- Extract tracks from dataset\n",
    "- Switch to mono\n",
    "- Apply various audio augmentations (pitch shifting, time stretching, compression, and reverb)\n",
    "  - May be modified/improved later\n",
    "- Saves augmented data to a given folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91b836",
   "metadata": {},
   "source": [
    "## Configuration and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b53b1",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and file operations\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Numerical and scientific computing\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "# Audio processing and manipulation\n",
    "import librosa  # Audio analysis library\n",
    "import soundfile as sf  # Audio file reading/writing\n",
    "import musdb  # MUSDB18 dataset handler for music source separation\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Jupyter notebook display utilities\n",
    "from IPython.display import Audio, display  # For playing and displaying audio in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "class Config:\n",
    "    # Paths\n",
    "    MUSDB18_PATH = \"/Users/agathe/Desktop/musdb18\"          \n",
    "    # Update this path to your MUSDB18 dataset\n",
    "\n",
    "    OUTPUT_DIR_COHERENT_MIX = \"/Users/agathe/Desktop/Augmented_Data_Coherent_Mix\"\n",
    "    # Update this path to your desired output directory for coherent mixes\n",
    "    \n",
    "    OUTPUT_DIR_INCOHERENT_MIX = \"/Users/agathe/Desktop/Augmented_Data_Incoherent_Mix\"\n",
    "    # Update this path to your desired output directory for incoherent mixes\n",
    "    \n",
    "    # Audio parameters\n",
    "    SAMPLE_RATE = 44100  # Standard sample rate for MUSDB18\n",
    "    \n",
    "    # Augmentation ranges\n",
    "    PITCH_SHIFT_RANGE = (-2, 2)             # Pitch shift in semitones\n",
    "    TIME_STRETCH_RANGE = (0.8, 1.2)         # Time stretching factor\n",
    "    # NOISE_LEVEL_RANGE = (0.001, 0.01)       # Noise level range not used in this example\n",
    "    GAIN_RANGE = (0.7, 1.3)                 # Gain adjustment range\n",
    "    \n",
    "    # Dynamic range compression parameters\n",
    "    COMPRESSION_THRESHOLD_RANGE = (0.3, 0.7)  # Threshold for compression (0.0 to 1.0)\n",
    "    COMPRESSION_RATIO_RANGE = (2.0, 6.0)      # Compression ratio (higher = more compression)\n",
    "    \n",
    "    # Reverb parameters\n",
    "    REVERB_ROOM_SIZE_RANGE = (0.2, 0.8)       # Room size parameter (0-1)\n",
    "    REVERB_DAMPING_RANGE = (0.2, 0.8)         # Damping parameter (0-1)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(config.OUTPUT_DIR_COHERENT_MIX, exist_ok=True)\n",
    "os.makedirs(config.OUTPUT_DIR_INCOHERENT_MIX, exist_ok=True)\n",
    "print(f\"Output directory for coherent data mix generation: {config.OUTPUT_DIR_COHERENT_MIX}\")\n",
    "print(f\"Output directory for incoherent data mix generation: {config.OUTPUT_DIR_INCOHERENT_MIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c698104",
   "metadata": {},
   "source": [
    "### Dataset Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSDB_PATH = \"/Users/agathe/Desktop/musdb18\"\n",
    "\n",
    "# Extracting the whole set\n",
    "mus = musdb.DB(root=MUSDB_PATH, is_wav=False, download=True)\n",
    "# is_wav=False means that the dataset is .mp4\n",
    "\n",
    "mus_train = musdb.DB(root=MUSDB_PATH, is_wav=False, download=True, subsets=\"train\", split='train')\n",
    "mus_valid = musdb.DB(root=MUSDB_PATH, is_wav=False, download=True, subsets=\"train\", split='valid')\n",
    "mus_test  = musdb.DB(root=MUSDB_PATH, is_wav=False, download=True, subsets=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Whole dataset loaded with {len(mus)} tracks.\")\n",
    "print(f\"Training set loaded with {len(mus_train)} tracks.\")\n",
    "print(f\"Validation set loaded with {len(mus_valid)} tracks.\")\n",
    "print(f\"Test set loaded with {len(mus_test)} tracks.\")\n",
    "\n",
    "# Print the first track's name and type\n",
    "print(f\"First track: {mus[0].name}, Type: {type(mus[0])}\")\n",
    "# Print the type of the elelemnts within the mus[0]\n",
    "print(f\"Type of elements in the first track: {type(mus[0].audio)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the different sr in mus.tracks\n",
    "sr_list = [track.rate for track in mus.tracks]\n",
    "print(f\"\\nUnique counts of sample rates in mus.tracks: {np.unique(sr_list)}\")\n",
    "print(f\"Sampling rates are matching: {config.SAMPLE_RATE == np.unique(sr_list)[0]}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fd003",
   "metadata": {},
   "source": [
    "### Track object visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_track_by_index(mus, k, to_mono=True):\n",
    "    \"\"\"\n",
    "    Extract the track at index k from the MUSDB18 dataset using musdb library,\n",
    "    with option to convert stereo audio to mono.\n",
    "    \n",
    "    MUSDB18 dataset typically contains stems for vocals, drums, bass, and other instruments,\n",
    "    along with the full mixture. Each track is organized with:\n",
    "    - track.audio: The full audio mixture (typically stereo)\n",
    "    - track.sources: Dictionary of individual source stems (vocals, drums, bass, other)\n",
    "    - track.targets: Processed version of stems used for evaluation\n",
    "\n",
    "    Args:\n",
    "        mus (musdb.DB): An instance of the musdb.DB loader containing the MUSDB18 dataset.\n",
    "                        Can be the full dataset, train set, or test set.\n",
    "        k (int): Index of the track to extract from the dataset (zero-based).\n",
    "        to_mono (bool): If True, converts all audio in the track to mono by averaging channels.\n",
    "                        This affects the mixture audio, all sources, and all targets.\n",
    "                        Default is True.\n",
    "\n",
    "    Returns:\n",
    "        musdb.Track: The musdb Track object at index k, with audio converted to mono if specified.\n",
    "                     The returned track maintains all its original properties but with modified audio arrays.\n",
    "    \n",
    "    Raises:\n",
    "        IndexError: If the requested track index is out of range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate that the track index is within the dataset bounds\n",
    "    if k < 0 or k >= len(mus.tracks):\n",
    "        raise IndexError(f\"Track index {k} is out of range (0, {len(mus.tracks)-1})\")\n",
    "    \n",
    "    # Get the track at the specified index\n",
    "    track = mus.tracks[k]\n",
    "    \n",
    "    # If to_mono is True, convert all audio components to mono\n",
    "    if to_mono:\n",
    "        # Convert main mixture audio to mono\n",
    "        # Audio in MUSDB18 has shape (n_samples, n_channels) where n_channels=2 for stereo\n",
    "        if track.audio.ndim > 1 and track.audio.shape[1] == 2:\n",
    "            # librosa.to_mono expects shape (n_channels, n_samples), so we transpose\n",
    "            # to_mono performs averaging across channels: (left+right)/2\n",
    "            mono_audio = librosa.to_mono(track.audio.T)  # Transpose to match librosa's expected format\n",
    "            track.audio = mono_audio  # Replace stereo with mono version (shape becomes 1D array of n_samples)\n",
    "        \n",
    "        # Convert all individual sources/stems to mono (vocals, drums, bass, other)\n",
    "        # Sources contain the raw, unprocessed stems\n",
    "        for source_name in track.sources:\n",
    "            source = track.sources[source_name]\n",
    "            if source.audio.ndim > 1 and source.audio.shape[1] == 2:\n",
    "                mono_source = librosa.to_mono(source.audio.T)\n",
    "                source.audio = mono_source  # Replace the stereo stem with mono version\n",
    "\n",
    "        # Also convert targets if they exist and are different from sources\n",
    "        # Targets are the processed versions of stems used for evaluation\n",
    "        for target_name in track.targets:\n",
    "            target = track.targets[target_name]\n",
    "            if target.audio.ndim > 1 and target.audio.shape[1] == 2:\n",
    "                mono_target = librosa.to_mono(target.audio.T)\n",
    "                target.audio = mono_target  # Replace the stereo target with mono version\n",
    "\n",
    "    # Return the track (either original stereo or converted to mono)\n",
    "    return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4663a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = extract_track_by_index(mus_train, 4)  # Extract the first track\n",
    "\n",
    "audio_data, sr = track.audio, track.rate\n",
    "audio_data = audio_data / np.max(np.abs(audio_data))  # Normalize audio data to [-1, 1] range\n",
    "\n",
    "print(f\"Track name: {track.name}\")\n",
    "print(f\"Sample rate: {sr}\")\n",
    "\n",
    "print(f\"audio_data shape: {audio_data.shape}\")\n",
    "print(f\"audio_data duration: {audio_data.shape[0] / sr:.2f} seconds\")\n",
    "print(f\"Details of sources: {track.sources}\")\n",
    "print(f\"Details of targets: {track.targets}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Sources names: {mus.sources_names}\")\n",
    "print(f\"Targets names: {mus.targets_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08cf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stems(track, sr=track.rate, max_duration=track.audio.shape[0] / track.rate):\n",
    "    \"\"\"\n",
    "    Plot waveforms and audio players for each source stem in a MUSDB18 track.\n",
    "    \n",
    "    Parameters:\n",
    "        track: musdb.Track\n",
    "        sr: sample rate (optional)\n",
    "        max_duration: max seconds to display/play per stem\n",
    "                     if None, uses the whole track duration\n",
    "    \"\"\"\n",
    "    if sr is None:\n",
    "        sr = track.rate\n",
    "\n",
    "    sources = track.targets.keys()\n",
    "    num_sources = len(sources)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_sources, 1, figsize=(14, 2 * num_sources), sharex=True)\n",
    "\n",
    "    print(f\"Showing up to {max_duration} seconds of {track.name} for: {', '.join(sources)}\")\n",
    "    print()\n",
    "\n",
    "    for i, stem_name in enumerate(sources):\n",
    "        audio = track.targets[stem_name].audio\n",
    "        audio = librosa.to_mono(audio.T)  # make mono for simplicity\n",
    "        audio = audio[:int(max_duration * sr)]  # crop to max_duration\n",
    "        \n",
    "        librosa.display.waveshow(audio, sr=sr, ax=axes[i])\n",
    "        axes[i].set_title(stem_name.capitalize())\n",
    "        axes[i].set_ylabel(\"Amplitude\")\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    axes[-1].set_xlabel(\"Time (s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.title(track.name)\n",
    "    plt.show()\n",
    "\n",
    "    # Audio players\n",
    "    # Looping through all sources except linear mixture\n",
    "    for stem_name in sources:\n",
    "        audio = track.targets[stem_name].audio\n",
    "        audio = librosa.to_mono(audio.T)\n",
    "        audio = audio[:int(max_duration * sr)]\n",
    "        print(f\"▶️ {stem_name.capitalize()}\")\n",
    "        display(Audio(audio, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stems(track)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f5e097",
   "metadata": {},
   "source": [
    "### Audio Extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio(track, stem='mixture', duration=10.0, offset = 30.0):\n",
    "    \"\"\"\n",
    "    Extract audio of a specific stem from a MUSDB18 track, starting from a random offset.\n",
    "    \n",
    "    Args:\n",
    "        track (musdb.Track): A musdb18 track object (mono or stereo)\n",
    "        stem (str): Which stem to extract:\n",
    "            - 'mixture': Extract from the full mixture audio\n",
    "            - 'vocals': Extract from the vocals stem\n",
    "            - 'drums': Extract from the drums stem\n",
    "            - 'bass': Extract from the bass stem\n",
    "            - 'other': Extract from the other stem\n",
    "        duration (float): Duration in seconds to extract\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Audio segment extracted from the specified stem at a random offset\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the specified stem is not available in the track\n",
    "    \"\"\"\n",
    "    # Get sample rate\n",
    "    sr = track.rate\n",
    "    \n",
    "    # Validate the stem parameter\n",
    "    if stem == 'mixture':\n",
    "        audio = track.audio\n",
    "    elif stem in track.sources:\n",
    "        audio = track.sources[stem].audio\n",
    "    else:\n",
    "        valid_stems = ['mixture'] + list(track.sources.keys())\n",
    "        raise ValueError(f\"Invalid stem '{stem}'. Valid options are: {', '.join(valid_stems)}\")\n",
    "    \n",
    "    # Ensure audio is mono (if not already)\n",
    "    if audio.ndim > 1:\n",
    "        audio = librosa.to_mono(audio.T)\n",
    "    \n",
    "    # Calculate maximum possible offset to ensure we can extract the full duration\n",
    "    total_duration = len(audio) / sr\n",
    "    max_offset = total_duration - duration\n",
    "    \n",
    "    # If the selected offset is greater than the maximum offset, raise an error\n",
    "    if offset > max_offset:\n",
    "        raise ValueError(f\"Offset {offset} seconds exceeds maximum possible offset of {max_offset:.2f} seconds for this track.\")\n",
    "    \n",
    "    # Convert offset to samples\n",
    "    offset_samples = int(offset * sr)\n",
    "    \n",
    "    # Extract the audio segment\n",
    "    end_sample = offset_samples + int(duration * sr)\n",
    "    audio = audio[offset_samples:end_sample]\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82923071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_with_melspec(audio, sr=44100, n_fft=2048, hop_length=512, n_mels=128, title=\"Audio Waveform and Mel Spectrogram\"):\n",
    "    \"\"\"\n",
    "    Plot the waveform of an audio signal.\n",
    "    \n",
    "    Parameters:\n",
    "        audio (np.ndarray): Audio signal array (1D for mono, 2D for stereo)\n",
    "        sr (int): Sample rate of the audio\n",
    "        title (str): Super Title for the plot\n",
    "    \"\"\"\n",
    "    if audio.ndim > 1:\n",
    "        raise ValueError(\"Audio must be a 1D array (mono) for waveform plotting. Make sure you used extract_audio().\")\n",
    "    \n",
    "    \n",
    "    # Create figure with two subplots (stacked vertically)\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot waveform on top subplot\n",
    "    librosa.display.waveshow(audio, sr=sr, ax=ax[0])\n",
    "    ax[0].set_title(\"Waveform\")\n",
    "    ax[0].set_ylabel(\"Amplitude\")\n",
    "    ax[0].grid(True)\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, \n",
    "        sr=sr, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    # Convert to dB scale for better visualization\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Plot mel spectrogram on bottom subplot\n",
    "    img = librosa.display.specshow(\n",
    "        mel_spec_db, \n",
    "        x_axis='time', \n",
    "        y_axis='mel', \n",
    "        sr=sr, \n",
    "        hop_length=hop_length, \n",
    "        ax=ax[1]\n",
    "    )\n",
    "    ax[1].set_title(\"Mel Spectrogram\")\n",
    "    ax[1].set_xlabel(\"Time (s)\")\n",
    "    ax[1].set_ylabel(\"Frequency (mel)\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(img, ax=ax[1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Make room for the title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_track = extract_track_by_index(mus_train, 8) \n",
    "\n",
    "my_audio = extract_audio(my_track, stem='vocals', duration=10.0, offset=30.0)\n",
    "my_audio = my_audio / np.max(np.abs(my_audio))  # Normalize audio to [-1, 1]\n",
    "\n",
    "print(f\"Extracted audio shape: {my_audio.shape}\")\n",
    "print(f\"Extracted audio duration: {my_audio.shape[0] / my_track.rate:.2f} seconds\")\n",
    "# Play the extracted audio\n",
    "display(Audio(my_audio, rate=my_track.rate))\n",
    "\n",
    "plot_audio_with_melspec(my_audio, sr=my_track.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation Functions\n",
    "\n",
    "Implementation of various audio augmentation techniques similar to those available in Scaper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08455c6b",
   "metadata": {},
   "source": [
    "### Pitch Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb83a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_shift_audio(audio, sr, n_steps=0, normalize=True):\n",
    "    \"\"\"\n",
    "    Apply pitch shifting to audio data in a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data as a mono numpy array, assumed to be 1D.\n",
    "        sr (int): Sample rate of the audio data\n",
    "        n_steps (float): Number of semitones to shift\n",
    "        normalize (bool): If True, normalizes the audio to the range [-1, 1] after pitch shifting.\n",
    "                          Default is True.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Pitch-shifted audio array\n",
    "                         \n",
    "    Raises:\n",
    "        ValueError: If audio is not a 1D numpy array (mono)\n",
    "    \"\"\"\n",
    "    # Check if audio is mono (1D array)\n",
    "    if audio.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio array (1D), but got multi-dimensional audio.\")\n",
    "    audio = audio / np.max(np.abs(audio))  # Normalize audio to [-1, 1] before processing\n",
    "    \n",
    "    # Apply pitch shifting to the mono audio array\n",
    "    shifted_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n",
    "\n",
    "    if normalize:\n",
    "        # Normalize the audio to the range [-1, 1]\n",
    "        shifted_audio = shifted_audio / np.max(np.abs(shifted_audio))\n",
    "    return shifted_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006cb943",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=6\n",
    "ps_my_audio = pitch_shift_audio(my_audio, sr=my_track.rate, n_steps=n_steps)\n",
    "print(f\"Pitch-shifted audio shape: {ps_my_audio.shape}\")\n",
    "\n",
    "# Play the pitch-shifted audio\n",
    "display(Audio(ps_my_audio, rate=my_track.rate))\n",
    "plot_audio_with_melspec(ps_my_audio, sr=my_track.rate, title=f\"Pitch-Shifted Audio Waveform, n_steps={n_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51227f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_comparison_with_melspec(original, transformed, sample_rate=44100, title=\"Audio Comparison\", \n",
    "                                     n_fft=2048, hop_length=512, n_mels=128, figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Plot waveforms and mel spectrograms for original and transformed audio side by side.\n",
    "    Both inputs are expected to be in mono format.\n",
    "    Both waveform plots will use the same time limits based on the transformed signal.\n",
    "    \n",
    "    Args:\n",
    "        original (np.array): Original audio array (mono)\n",
    "        transformed (np.array): Transformed audio array (mono)\n",
    "        sample_rate (int): Sample rate of the audio\n",
    "        title (str): Main title for the plot\n",
    "        n_fft (int): FFT window size for mel spectrogramx\n",
    "        hop_length (int): Hop length for mel spectrogram\n",
    "        n_mels (int): Number of mel bands to generate\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "    \"\"\"\n",
    "    # Create figure with subplots: 2 rows (waveform, mel spectrogram) and 2 columns (original, transformed)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Ensure both inputs are 1D arrays (mono)\n",
    "    if original.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio for 'original', but got multi-dimensional array\")\n",
    "    if transformed.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio for 'transformed', but got multi-dimensional array\")\n",
    "    \n",
    "    # Calculate time arrays for waveforms\n",
    "    time_orig = np.arange(len(original)) / sample_rate\n",
    "    time_trans = np.arange(len(transformed)) / sample_rate\n",
    "    \n",
    "    # Get the duration of the transformed signal\n",
    "    trans_duration = len(transformed) / sample_rate\n",
    "    \n",
    "    # Plot waveforms\n",
    "    axs[0, 0].plot(time_orig, original)\n",
    "    axs[0, 0].set_title(\"Original Waveform\")\n",
    "    axs[0, 0].set_xlabel(\"Time (s)\")\n",
    "    axs[0, 0].set_ylabel(\"Amplitude\")\n",
    "    axs[0, 0].grid(True)\n",
    "    # Set x-axis limits to match the transformed signal duration\n",
    "    axs[0, 0].set_xlim(0, trans_duration)\n",
    "    \n",
    "    axs[0, 1].plot(time_trans, transformed)\n",
    "    axs[0, 1].set_title(\"Transformed Waveform\")\n",
    "    axs[0, 1].set_xlabel(\"Time (s)\")\n",
    "    axs[0, 1].set_ylabel(\"Amplitude\")\n",
    "    axs[0, 1].grid(True)\n",
    "    # Set x-axis limits to match the transformed signal duration\n",
    "    axs[0, 1].set_xlim(0, trans_duration)\n",
    "    \n",
    "    # Compute mel spectrograms\n",
    "    mel_orig = librosa.feature.melspectrogram(\n",
    "        y=original, \n",
    "        sr=sample_rate, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    mel_trans = librosa.feature.melspectrogram(\n",
    "        y=transformed, \n",
    "        sr=sample_rate, \n",
    "        n_fft=n_fft, \n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    S_orig_db = librosa.power_to_db(mel_orig, ref=np.max)\n",
    "    S_trans_db = librosa.power_to_db(mel_trans, ref=np.max)\n",
    "    \n",
    "    # Plot mel spectrograms\n",
    "    img1 = librosa.display.specshow(\n",
    "        S_orig_db, \n",
    "        x_axis='time', \n",
    "        y_axis='mel', \n",
    "        sr=sample_rate, \n",
    "        hop_length=hop_length, \n",
    "        ax=axs[1, 0]\n",
    "    )\n",
    "    axs[1, 0].set_title('Original Mel Spectrogram')\n",
    "    # Set x-axis limits for spectrogram to match transformed duration\n",
    "    axs[1, 0].set_xlim(0, trans_duration)\n",
    "    \n",
    "    img2 = librosa.display.specshow(\n",
    "        S_trans_db, \n",
    "        x_axis='time', \n",
    "        y_axis='mel', \n",
    "        sr=sample_rate, \n",
    "        hop_length=hop_length, \n",
    "        ax=axs[1, 1]\n",
    "    )\n",
    "    axs[1, 1].set_title('Transformed Mel Spectrogram')\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(img1, ax=axs[1, 0], format='%+2.0f dB')\n",
    "    fig.colorbar(img2, ax=axs[1, 1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)  # Adjust to make room for the main title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5fa8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_audio_comparison_with_melspec(my_audio, ps_my_audio, sample_rate=my_track.rate, title=f\"Original vs Pitch-Shifted Audio n_steps={n_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4f723",
   "metadata": {},
   "source": [
    "### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stretch_audio(audio, stretch_factor=1.0, normalize=True):\n",
    "    \"\"\"\n",
    "    Apply time stretching to audio data in a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data as a mono numpy array\n",
    "        stretch_factor (float): Time stretch factor:\n",
    "                               - stretch_factor > 1.0: slower/longer audio\n",
    "                               - stretch_factor < 1.0: faster/shorter audio\n",
    "                               - stretch_factor = 1.0: unchanged (default)\n",
    "        normalize (bool): If True, normalizes the audio to the range [-1, 1] after time stretching.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Time-stretched audio array\n",
    "                         \n",
    "    Raises:\n",
    "        ValueError: If audio is not a 1D numpy array (mono)\n",
    "    \"\"\"\n",
    "    # Check if audio is mono (1D array)\n",
    "    if audio.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio array (1D), but got multi-dimensional audio. \")\n",
    "    \n",
    "    audio = audio / np.max(np.abs(audio))  # Normalize audio to [-1, 1] before processing\n",
    "    \n",
    "    # Apply time stretching to the mono audio array\n",
    "    # Note: librosa.effects.time_stretch takes rate parameter where:\n",
    "        # rate > 1 makes audio faster (shorter), rate < 1 makes audio slower (longer)\n",
    "    # This is the inverse of our stretch_factor parameter, so we use 1/stretch_factor\n",
    "\n",
    "    ts_audio = librosa.effects.time_stretch(audio, rate=1/stretch_factor)\n",
    "    # Normalize the audio to [-1, 1] if requested\n",
    "    if normalize:\n",
    "        ts_audio = ts_audio / np.max(np.abs(ts_audio))\n",
    "    return ts_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75756df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch_factor = 1.4\n",
    "ts_my_audio = time_stretch_audio(my_audio, stretch_factor=stretch_factor, normalize=True)\n",
    "\n",
    "print(f\"Time-stretched audio shape: {ts_my_audio.shape}\")\n",
    "# Play the time-stretched audio\n",
    "display(Audio(ts_my_audio, rate=my_track.rate))\n",
    "plot_audio_comparison_with_melspec(my_audio, ts_my_audio, sample_rate=my_track.rate, \n",
    "                                     title=f\"Original vs Time-Stretched Audio (Factor: {stretch_factor})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f8782",
   "metadata": {},
   "source": [
    "### Add Noise (Not considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_noise_audio(audio, noise_level=0.001, normalize=True):\n",
    "#     \"\"\"\n",
    "#     Add Gaussian noise to audio data in a numpy array.\n",
    "    \n",
    "#     Args:\n",
    "#         audio (np.ndarray): Audio data as a mono numpy array\n",
    "#         sr (int): Sample rate of the audio data\n",
    "#         noise_level (float): Standard deviation of noise to add\n",
    "#         normalize (bool): If True, normalizes the audio to the range [-1, 1] after adding noise.\n",
    "    \n",
    "#     Returns:\n",
    "#         np.ndarray: Noise-added audio array\n",
    "                         \n",
    "#     Raises:\n",
    "#         ValueError: If audio is not a 1D numpy array (mono)\n",
    "#     \"\"\"\n",
    "#     # Check if audio is mono (1D array)\n",
    "#     if audio.ndim > 1:\n",
    "#         raise ValueError(\"Expected mono audio array (1D), but got multi-dimensional audio.\")\n",
    "    \n",
    "#     audio = audio / np.max(np.abs(audio))  # Normalize audio to [-1, 1] before processing\n",
    "#     # Generate noise matching the audio shape\n",
    "#     noise = np.random.normal(0, noise_level, audio.shape)\n",
    "    \n",
    "#     # Add noise to audio\n",
    "#     noise_audio = audio + noise\n",
    "#\n",
    "#     # Normalize the audio to [-1, 1] if requested\n",
    "#     if normalize:\n",
    "#         noise_audio = noise_audio / np.max(np.abs(noise_audio))\n",
    "#\n",
    "#     return noise_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b6b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_level = 0.005  # Standard deviation of noise\n",
    "\n",
    "# noise_my_audio = add_noise_audio(my_audio, noise_level=noise_level, normaize=True)\n",
    "# display(Audio(noise_my_audio, rate=my_track.rate))\n",
    "\n",
    "# plot_audio_comparison_with_melspec(my_audio, noise_my_audio, sample_rate=my_track.rate,\n",
    "#                                      title=f\"Original vs Noise-Added Audio (Noise Level: {noise_level})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d2ea9",
   "metadata": {},
   "source": [
    "### Dynamic Range Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_dynamic_range_audio(audio, threshold=0.2, ratio=2.0, normalize=True):\n",
    "    \"\"\"\n",
    "    Apply dynamic range compression to audio data in a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data as a mono numpy array\n",
    "        sr (int): Sample rate of the audio data\n",
    "        threshold (float): Compression threshold (0.0 to 1.0)\n",
    "        ratio (float): Compression ratio (>1.0, higher = more compression)\n",
    "                      This is the amount of gain reduction applied to signals above the threshold \n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Compressed audio array\n",
    "                         \n",
    "    Raises:\n",
    "        ValueError: If audio is not a 1D numpy array (mono)\n",
    "    \"\"\"\n",
    "    # Check if audio is mono (1D array)\n",
    "    if audio.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio array (1D), but got multi-dimensional audio.\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    compressed_audio = audio.copy()\n",
    "\n",
    "    compressed_audio = compressed_audio / np.max(np.abs(compressed_audio))  # Normalize audio to [-1, 1] before processing\n",
    "    \n",
    "    # Calculate absolute values for threshold comparison\n",
    "    abs_audio = np.abs(compressed_audio)\n",
    "    \n",
    "    # Find samples above the threshold\n",
    "    above_threshold = abs_audio > threshold\n",
    "    \n",
    "    # Apply compression to samples above threshold\n",
    "    if np.any(above_threshold):\n",
    "        # Calculate the gain reduction for samples above threshold\n",
    "        # Formula: output = threshold + (input - threshold) / ratio\n",
    "        excess = abs_audio[above_threshold] - threshold\n",
    "        compressed_excess = excess / ratio\n",
    "        new_amplitude = threshold + compressed_excess\n",
    "        \n",
    "        # Apply the compression while preserving the sign\n",
    "        gain_factor = new_amplitude / abs_audio[above_threshold]\n",
    "        compressed_audio[above_threshold] *= gain_factor\n",
    "    \n",
    "    if normalize:\n",
    "        # Normalize the compressed audio to [-1, 1]\n",
    "        compressed_audio = compressed_audio / np.max(np.abs(compressed_audio))\n",
    "    \n",
    "    return compressed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca682bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6  # Compression threshold\n",
    "ratio = 12.0  # Compression ratio\n",
    "\n",
    "com_my_audio = compress_dynamic_range_audio(my_audio, threshold=threshold, ratio=ratio, normalize=True)\n",
    "print(f\"Compressed audio shape: {com_my_audio.shape}\")\n",
    "# Play the compressed audio\n",
    "print(f'Original audio:')\n",
    "display(Audio(my_audio, rate=my_track.rate))\n",
    "print(f'Compressed audio:')\n",
    "display(Audio(com_my_audio, rate=my_track.rate))\n",
    "plot_audio_comparison_with_melspec(my_audio, com_my_audio, sample_rate=my_track.rate,\n",
    "                                     title=f\"Original vs Compressed Audio (Threshold: {threshold}, Ratio: {ratio})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3006959",
   "metadata": {},
   "source": [
    "### Reverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reverb_audio(audio, sr, room_size=0.5, damping=0.5, normalize=True):\n",
    "    \"\"\"\n",
    "    Apply simple reverb effect to audio data in a numpy array.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data as a mono numpy array\n",
    "        sr (int): Sample rate of the audio data\n",
    "        room_size (float): Room size parameter (0-1)\n",
    "                          Higher values create longer reverb tails\n",
    "        damping (float): Damping parameter (0-1) \n",
    "                        Higher values create faster decay of reverb\n",
    "        normalize (bool): If True, normalizes the audio to the range [-1, 1] after adding reverb.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Reverb-added audio array\n",
    "                         \n",
    "    Raises:\n",
    "        ValueError: If audio is not a 1D numpy array (mono)\n",
    "    \"\"\"\n",
    "    # Check if audio is mono (1D array)\n",
    "    if audio.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio array (1D), but got multi-dimensional audio.\")\n",
    "    \n",
    "    audio = audio / np.max(np.abs(audio))  # Normalize audio to [-1, 1] before processing\n",
    "\n",
    "    # Create a simple impulse response for reverb\n",
    "    ir_length = int(sr * room_size)  # Impulse response length\n",
    "    \n",
    "    # Generate exponentially decaying noise as impulse response\n",
    "    decay = np.exp(-np.arange(ir_length) * damping * 10 / ir_length)\n",
    "    impulse_response = np.random.normal(0, 0.1, ir_length) * decay\n",
    "    \n",
    "    # Apply convolution for mono audio\n",
    "    convolved = np.convolve(audio, impulse_response, mode='full')\n",
    "    reverb_audio = convolved[:len(audio)]\n",
    "    \n",
    "    # Mix with original audio (30% reverb, 70% original)\n",
    "    res = 0.7 * audio + 0.3 * reverb_audio\n",
    "\n",
    "    if normalize:\n",
    "        res = res / np.max(np.abs(res))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed514e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb_room_size = 0.8  # Room size parameter (0-1)\n",
    "reverb_damping = 0.6  # Damping parameter (0-1)\n",
    "reverb_my_audio = apply_reverb_audio(my_audio, sr=my_track.rate, \n",
    "                                      room_size=reverb_room_size, damping=reverb_damping, normalize=True)\n",
    "print(f\"Reverb audio shape: {reverb_my_audio.shape}\")\n",
    "# Play the reverb audio\n",
    "display(Audio(reverb_my_audio, rate=my_track.rate))\n",
    "plot_audio_comparison_with_melspec(my_audio, reverb_my_audio, sample_rate=my_track.rate,\n",
    "                                     title=f\"Original vs Reverb Audio (Room Size: {reverb_room_size}, Damping: {reverb_damping})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Function To Rule Them All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentations(audio, sr, augmentation_types, config=config):\n",
    "    \"\"\"\n",
    "    Apply specified augmentations to audio data.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data as a mono numpy array\n",
    "        sr (int): Sample rate of the audio data\n",
    "        augmentation_types (list): A list of strings specifying augmentation types to apply\n",
    "            Valid types: 'pitch_shift', 'time_stretch', 'compression', 'reverb', 'noise'\n",
    "        config: Optional configuration object with augmentation parameters.\n",
    "            If None, uses reasonable default ranges for parameters.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (augmented_audio, augmentation_description)\n",
    "              augmented_audio is the processed numpy array\n",
    "              augmentation_description is a string describing what was applied\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If augmentation_types is not a list of strings\n",
    "        ValueError: If an invalid augmentation type is specified\n",
    "        ValueError: If audio is not a 1D numpy array (mono)\n",
    "    \"\"\"\n",
    "    # Check if audio is mono (1D array)\n",
    "    if audio.ndim > 1:\n",
    "        raise ValueError(\"Expected mono audio array (1D), but got multi-dimensional audio.\")\n",
    "    \n",
    "    # Validate augmentation_types parameter type\n",
    "    valid_augmentation_types = ['pitch_shift', 'time_stretch', 'compression', 'reverb', 'noise']\n",
    "    \n",
    "    if not isinstance(augmentation_types, list) or not all(isinstance(aug, str) for aug in augmentation_types):\n",
    "        raise ValueError(\"augmentation_types must be a list of strings specifying augmentation types. Valid options are: \" + \", \".join(valid_augmentation_types))\n",
    "    \n",
    "    # Store augmentation choices\n",
    "    applied_augmentations = []\n",
    "    \n",
    "\n",
    "    # Set up default parameter ranges if config is not provided\n",
    "    if config is None:\n",
    "        raise ValueError(\"Config object must be provided with augmentation parameters.\")\n",
    "    \n",
    "    # Set up augmentation choices based on input parameter\n",
    "    augmentation_choices = {aug: False for aug in valid_augmentation_types}\n",
    "\n",
    "    # Process the list of augmentation types\n",
    "    for aug_type in augmentation_types:\n",
    "        if aug_type not in valid_augmentation_types:\n",
    "            raise ValueError(f\"Invalid augmentation type: '{aug_type}'. Valid options are: {', '.join(valid_augmentation_types)}\")\n",
    "        augmentation_choices[aug_type] = True\n",
    "    \n",
    "    # Start with the original audio\n",
    "    result = audio.copy()\n",
    "    \n",
    "    # Apply pitch shift\n",
    "    if augmentation_choices['pitch_shift']:\n",
    "        n_steps = random.uniform(*config.PITCH_SHIFT_RANGE)\n",
    "        result = pitch_shift_audio(result, sr=sr, n_steps=n_steps, normalize=True)\n",
    "        applied_augmentations.append(f\"Pitch shift: {n_steps:.2f} semitones\")\n",
    "    \n",
    "    # Apply time stretch\n",
    "    if augmentation_choices['time_stretch']:\n",
    "        stretch_factor = random.uniform(*config.TIME_STRETCH_RANGE)\n",
    "        result = time_stretch_audio(result, stretch_factor=stretch_factor, normalize=True)\n",
    "        applied_augmentations.append(f\"Time stretch: {stretch_factor:.2f}x\")\n",
    "    \n",
    "    # # Apply noise\n",
    "    # if augmentation_choices['noise']:\n",
    "    #     noise_level = random.uniform(*config.NOISE_LEVEL_RANGE)\n",
    "    #     result = add_noise_audio(result, noise_level=noise_level, normalize=True)\n",
    "    #     applied_augmentations.append(f\"Noise: {noise_level:.4f} std dev\")\n",
    "    \n",
    "    # Apply compression\n",
    "    if augmentation_choices['compression']:\n",
    "        threshold = random.uniform(*config.COMPRESSION_THRESHOLD_RANGE)\n",
    "        ratio = random.uniform(*config.COMPRESSION_RATIO_RANGE)\n",
    "        result = compress_dynamic_range_audio(result, threshold=threshold, ratio=ratio, normalize=True )\n",
    "        applied_augmentations.append(f\"Compression: {ratio:.1f}:1 @ {threshold:.2f}\")\n",
    "    \n",
    "    # Apply reverb\n",
    "    if augmentation_choices['reverb']:\n",
    "        room_size = random.uniform(*config.REVERB_ROOM_SIZE_RANGE)\n",
    "        damping = random.uniform(*config.REVERB_DAMPING_RANGE)\n",
    "        result = apply_reverb_audio(result, sr=sr, room_size=room_size, damping=damping, normalize=True)\n",
    "        applied_augmentations.append(f\"Reverb: room={room_size:.2f}, damp={damping:.2f}\")\n",
    "    \n",
    "    # Normalize to prevent clipping\n",
    "    result = result / np.max(np.abs(result))  # Normalize audio to [-1, 1] range\n",
    "    \n",
    "    description = \"; \".join(applied_augmentations) if applied_augmentations else \"No augmentation\"\n",
    "    \n",
    "    return result, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_types = ['pitch_shift', 'time_stretch', 'compression', 'reverb']\n",
    "\n",
    "\n",
    "aug_my_audio, aug_description = apply_augmentations(my_audio, sr=my_track.rate, augmentation_types=augmentation_types, config=config)\n",
    "print(f\"Augmented audio shape: {aug_my_audio.shape}\")\n",
    "# Play the augmented audio\n",
    "display(Audio(aug_my_audio, rate=my_track.rate))\n",
    "plot_audio_comparison_with_melspec(my_audio, aug_my_audio, sample_rate=my_track.rate,\n",
    "                                     title=f\"Original vs Augmented Audio\\n({aug_description})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a17f9",
   "metadata": {},
   "source": [
    "## Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b3b4f9",
   "metadata": {},
   "source": [
    "### Audio Suitability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(audio, energy_threshold=0.005, silent_threshold=0.002, min_active_ratio=0.3, window_size=4410):\n",
    "    \"\"\"\n",
    "    Check if an audio array has enough energy and sufficient non-silent segments to be considered suitable for processing.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio data as a mono numpy array\n",
    "        energy_threshold (float): Minimum overall RMS energy threshold (0.0 to 1.0)\n",
    "        silent_threshold (float): RMS energy threshold to consider a segment non-silent (0.0 to 1.0)\n",
    "        min_active_ratio (float): Minimum ratio of non-silent segments required (0.0 to 1.0)\n",
    "        window_size (int): Size of windows to analyze for silent detection (in samples)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the audio has enough energy and non-silent segments, False otherwise\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If audio is not a 1D numpy array (mono)\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    audio_check = audio.copy()\n",
    "    \n",
    "    # # Normalize if requested\n",
    "    # if normalize and np.max(np.abs(audio_check)) > 0:\n",
    "    #     audio_check = audio_check / np.max(np.abs(audio_check))\n",
    "    \n",
    "    # Calculate overall RMS energy\n",
    "    overall_rms = np.sqrt(np.mean(audio_check**2))\n",
    "    \n",
    "    # If overall energy is too low, return False immediately\n",
    "    if overall_rms <= energy_threshold:\n",
    "        return False\n",
    "    \n",
    "    # Check for non-silent segments\n",
    "    num_windows = len(audio_check) // window_size\n",
    "    \n",
    "    # If audio is shorter than one window, just use the overall RMS\n",
    "    if num_windows == 0:\n",
    "        return overall_rms > silent_threshold\n",
    "    \n",
    "    # Count non-silent segments\n",
    "    non_silent_count = 0\n",
    "    for i in range(num_windows):\n",
    "        start = i * window_size\n",
    "        end = start + window_size\n",
    "        segment = audio_check[start:end]\n",
    "        segment_rms = np.sqrt(np.mean(segment**2))\n",
    "        \n",
    "        if segment_rms > silent_threshold:\n",
    "            non_silent_count += 1\n",
    "    \n",
    "    # Calculate ratio of non-silent segments\n",
    "    active_ratio = non_silent_count / num_windows\n",
    "    \n",
    "    # Return True if both energy and non-silent criteria are met\n",
    "    return active_ratio >= min_active_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fee374",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_suitable_result = is_suitable(my_audio, energy_threshold=0.005, silent_threshold=0.002, min_active_ratio=0.3, window_size=4410)\n",
    "print(f\"Is the audio suitable for processing? {'Yes' if is_suitable_result else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f74027",
   "metadata": {},
   "source": [
    "### Semi Coherent Augmentation - Coherent Augmentation With Varying Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherent_augmentation_varying_params(mus_train, idx, config=config, duration=10.0, show_audio=False, max_attempts=10):\n",
    "    \"\"\"\n",
    "    Select a random track from the MUSDB18 training set, extract its stems,\n",
    "    and apply random augmentations to each stem. Ensures all stems are suitable\n",
    "    (have enough energy and non-silent segments) before processing.\n",
    "    \n",
    "    Args:\n",
    "        mus_train (musdb.DB): MUSDB18 training set object\n",
    "        config: Configuration object for augmentation parameters\n",
    "        duration (float): Duration in seconds to process for each stem\n",
    "        show_audio (bool): If True, display audio players for original and augmented stems\n",
    "        max_attempts (int): Maximum number of attempts to find suitable stem segments\n",
    "\n",
    "    Returns:\n",
    "        tuple: (original_stems, augmented_stems)\n",
    "            original_stems: {\n",
    "                'vocals': np.ndarray (if present),\n",
    "                'bass': np.ndarray (if present),\n",
    "                'drums': np.ndarray (if present),\n",
    "                'other': np.ndarray (if present),\n",
    "                'mixture': np.ndarray,  # Combined original stems\n",
    "                'track_name': str  # Name of the track\n",
    "            }\n",
    "            augmented_stems: {\n",
    "                'vocals': np.ndarray (if present),\n",
    "                'bass': np.ndarray (if present),\n",
    "                'drums': np.ndarray (if present),\n",
    "                'other': np.ndarray (if present),\n",
    "                'mixture': np.ndarray,  # Combined augmented stems\n",
    "                'descriptions': {'vocals': str, 'bass': str, 'drums': str, 'other': str},\n",
    "                'track_name': str  # Name of the track with '_aug' suffix\n",
    "            }\n",
    "    \"\"\"\n",
    "    if idx > len(mus_train.tracks) -1:\n",
    "        raise ValueError(f\"Index {idx} is out of bounds for the training set with {len(mus_train.tracks)} tracks.\")\n",
    "\n",
    "    # Extract the random track\n",
    "    track = extract_track_by_index(mus_train, idx, to_mono=True)\n",
    "\n",
    "    # Get sample rate\n",
    "    sr = track.rate\n",
    "    print(f\"Selected random track: {track.name} (index {idx})\")\n",
    "\n",
    "    # =========================== #\n",
    "    # VALIDATE DURATION PARAMETER #\n",
    "    # =========================== #\n",
    "\n",
    "    if duration is None:\n",
    "        raise ValueError(\"Duration must be specified for coherent augmentation.\")\n",
    "    if duration <= 0:\n",
    "        raise ValueError(\"Duration must be a positive number.\")\n",
    "    if duration > track.audio.shape[0] / sr:\n",
    "        raise ValueError(f\"Duration {duration} seconds exceeds track length {track.audio.shape[0] / sr:.2f} seconds.\")\n",
    "    if duration < 1.0:\n",
    "        raise ValueError(\"Duration must be at least 1 second for coherent augmentation.\")\n",
    "    \n",
    "    # =========================== #\n",
    "    # CHECKING STEMS AVAILABILITY #\n",
    "    # =========================== #\n",
    "\n",
    "    # Standard stem names in MUSDB18\n",
    "    stem_names = ['vocals', 'bass', 'drums', 'other']\n",
    "\n",
    "    # Check if the track has all the required stems\n",
    "    missing_stems = [stem for stem in stem_names if stem not in track.sources]\n",
    "    if missing_stems:\n",
    "        raise ValueError(f\"Track '{track.name}' is missing the following stems: {', '.join(missing_stems)}\")\n",
    "\n",
    "    # =============================== #\n",
    "    # FIND SUITABLE SEGMENT FOR STEMS #\n",
    "    # =============================== #\n",
    "    \n",
    "    # Calculate maximum possible offset to ensure we can extract the full duration\n",
    "    max_offset = track.audio.shape[0] / sr - duration\n",
    "    \n",
    "    # Initialize variables\n",
    "    all_stems_suitable = False\n",
    "    attempt_count = 0\n",
    "    \n",
    "    while not all_stems_suitable and attempt_count < max_attempts:\n",
    "        attempt_count += 1\n",
    "        print(f\"Attempt {attempt_count}/{max_attempts} to find suitable stems segment...\")\n",
    "        \n",
    "        # Select a random offset time to start processing\n",
    "        offset = random.uniform(0, max_offset)\n",
    "        start_sample = int(offset * sr)\n",
    "        end_sample = start_sample + int(duration * sr)\n",
    "        \n",
    "        # Check if all stems in this segment are suitable\n",
    "        stems_suitability = {}\n",
    "        all_suitable = True\n",
    "        \n",
    "        for stem in stem_names:\n",
    "            # Get the original audio for this stem\n",
    "            original_audio = track.sources[stem].audio[start_sample:end_sample]\n",
    "            # Check if this stem segment is suitable\n",
    "            stems_suitability[stem] = is_suitable(original_audio, energy_threshold=0.005, silent_threshold=0.002,\n",
    "                                                    min_active_ratio=0.3,window_size=4410)\n",
    "\n",
    "            if not stems_suitability[stem]:\n",
    "                all_suitable = False\n",
    "        \n",
    "        # Print suitability results for this attempt\n",
    "        print(f\"Segment at offset {offset:.2f}s:\")\n",
    "        for stem, suitable in stems_suitability.items():\n",
    "            print(f\"  - {stem}: {'Suitable' if suitable else 'Not suitable'}\")\n",
    "        \n",
    "        if all_suitable:\n",
    "            all_stems_suitable = True\n",
    "            print(f\"Found suitable segment after {attempt_count} attempts at offset {offset:.2f}s\")\n",
    "        elif attempt_count == max_attempts:\n",
    "            print(f\"Warning: Could not find segment with all suitable stems after {max_attempts} attempts.\")\n",
    "            print(\"Proceeding with the last segment checked.\")\n",
    "    \n",
    "    # ============================== #\n",
    "    # INITIALIZE RETURN DICTIONARIES #\n",
    "    # ============================== #\n",
    "\n",
    "    original_stems = {}\n",
    "    augmented_stems = {}\n",
    "\n",
    "    augmented_stems['descriptions'] = {}\n",
    "    original_stems['track_name'] = track.name\n",
    "    augmented_stems['track_name'] = track.name + \"_aug\"\n",
    "\n",
    "    # ================================= #\n",
    "    # APPLY AUGMENTATIONS FOR ALL STEMS #\n",
    "    # ================================= #\n",
    "    for stem in stem_names:\n",
    "        # Get the original audio for this stem\n",
    "        original_audio = track.sources[stem].audio[start_sample:end_sample]\n",
    "        original_audio = original_audio / np.max(np.abs(original_audio))  # Normalize audio to [-1, 1]\n",
    "\n",
    "        # Create a random list of augmentations to apply\n",
    "        random_augmentations = []\n",
    "\n",
    "        # 60% chance to add each augmentation type\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('pitch_shift')\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('time_stretch')\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('compression')\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('reverb')\n",
    "        # if random.random() < 0.6:\n",
    "        #     random_augmentations.append('noise')\n",
    "\n",
    "        # Apply the augmentations\n",
    "        augmented_audio, description = apply_augmentations(audio=original_audio,sr=sr,augmentation_types=random_augmentations,config=config)\n",
    "\n",
    "        # Pad the signal if less than the desired duration\n",
    "        if len(augmented_audio) < int(duration * sr):\n",
    "            padding_length = int(duration * sr) - len(augmented_audio)\n",
    "            augmented_audio = np.pad(augmented_audio, (0, padding_length), mode='constant')\n",
    "        \n",
    "        # Crop the signal if longer than the desired duration\n",
    "        if len(augmented_audio) > int(duration * sr):\n",
    "            augmented_audio = augmented_audio[:int(duration * sr)]\n",
    "\n",
    "        augmented_audio = augmented_audio / np.max(np.abs(augmented_audio))  # Normalize audio to [-1, 1]\n",
    "\n",
    "        # Store in return dictionaries\n",
    "        original_stems[stem] = original_audio\n",
    "        augmented_stems[stem] = augmented_audio[:int(duration * sr)]  # Ensure we only keep the duration we want\n",
    "        augmented_stems['descriptions'][stem] = description\n",
    "\n",
    "        if show_audio:\n",
    "            print(f\"Stem: {stem}\")\n",
    "            display(Audio(original_audio, rate=sr))\n",
    "            print(f\"Augmentation: {description}\")\n",
    "            display(Audio(augmented_audio, rate=sr))\n",
    "            print()\n",
    "\n",
    "    # ================================================================= #\n",
    "    # COMBINE ALL ORIGINAL  AND AUGMENTED STEMS INTO 2 DIFFERENT ARRAYS # \n",
    "    # ================================================================= #\n",
    "\n",
    "    original_combined = np.sum([(original_stems[stem]) for stem in stem_names if stem in original_stems], axis=0) /4\n",
    "\n",
    "\n",
    "\n",
    "    # Summing all the augmented stems into one array\n",
    "    print(f\"Augmented stems: {', '.join(augmented_stems.keys())}\")\n",
    "    for stem in stem_names:\n",
    "        print(f\"Augmented stem '{stem}' shape: {augmented_stems[stem].shape}\")\n",
    "    augmented_combined = np.sum([(augmented_stems[stem]) for stem in stem_names if stem in augmented_stems], axis=0) / 4\n",
    "\n",
    "    # Divide by 4 to prevent clipping, as we are summing multiple stems\n",
    "\n",
    "    # Add both combined mixtures to the results\n",
    "    original_stems['mixture'] = original_combined\n",
    "    augmented_stems['mixture'] = augmented_combined\n",
    "\n",
    "    # =================================== #\n",
    "    # DISPLAY COMBINED AUDIO IF REQUESTED #\n",
    "    # =================================== #\n",
    "    if show_audio:  \n",
    "        print(\"\\nOriginal mixture of all stems:\")\n",
    "        display(Audio(original_combined, rate=sr))\n",
    "\n",
    "        print(\"\\nAugmented mixture of all stems:\")\n",
    "        display(Audio(augmented_combined, rate=sr))\n",
    "\n",
    "    print(\"Augmentation complete\")\n",
    "    return original_stems, augmented_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_stems, aug_stems = coherent_augmentation_varying_params(mus_train, idx=1, config=config, duration=10.0, show_audio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806efdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Track name: {orig_stems['track_name']}\")\n",
    "print(f\"Augmented Track name: {aug_stems['track_name']}\\n\")\n",
    "\n",
    "print(\"Original stems:\")\n",
    "for stem, audio in orig_stems.items():\n",
    "    if stem != 'track_name':\n",
    "        print(f\"{stem.capitalize()}: {audio.shape} samples\")\n",
    "print(\"\\nAugmented stems:\")\n",
    "for stem, audio in aug_stems.items():\n",
    "    if stem != 'mixture' and stem != 'descriptions' and stem != 'track_name':\n",
    "        print(f\"{stem.capitalize()}: {audio.shape} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"orig_stems.keys(): {orig_stems.keys()}\")\n",
    "print(f\"aug_stems.keys(): {aug_stems.keys()}\")\n",
    "print(f\"aug_stems['descriptions']: {aug_stems['descriptions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stem in aug_stems.keys():\n",
    "    if stem != 'descriptions' and stem != 'track_name':\n",
    "        plot_audio_comparison_with_melspec(\n",
    "            orig_stems[stem], \n",
    "            aug_stems[stem], \n",
    "            sample_rate=44100, \n",
    "            title=f\"Original vs Augmented {stem.capitalize()} Audio\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ecd52c",
   "metadata": {},
   "source": [
    "### Coherent Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherent_augmentation_fixed_params(mus_train, idx, config, duration=10.0, show_audio=False, max_attempts=10):\n",
    "    \"\"\"\n",
    "    Select a random track from the MUSDB18 training set, extract its stems,\n",
    "    and apply the SAME random augmentations to each stem (fixed parameters).\n",
    "    Ensures all stems are suitable (have enough energy and non-silent segments) before processing.\n",
    "    \n",
    "    Args:\n",
    "        mus_train (musdb.DB): MUSDB18 training set object\n",
    "        idx (int): Index of the track to extract from the dataset\n",
    "        config: Configuration object for augmentation parameters\n",
    "        duration (float): Duration in seconds to process for each stem\n",
    "        show_audio (bool): If True, display audio players for original and augmented stems\n",
    "        max_attempts (int): Maximum number of attempts to find suitable stem segments\n",
    "\n",
    "    Returns:\n",
    "        tuple: (original_stems, augmented_stems)\n",
    "            original_stems: {\n",
    "                'vocals': np.ndarray (if present),\n",
    "                'bass': np.ndarray (if present),\n",
    "                'drums': np.ndarray (if present),\n",
    "                'other': np.ndarray (if present),\n",
    "                'mixture': np.ndarray,  # Combined original stems\n",
    "                'track_name': str  # Name of the track\n",
    "            }\n",
    "            augmented_stems: {\n",
    "                'vocals': np.ndarray (if present),\n",
    "                'bass': np.ndarray (if present),\n",
    "                'drums': np.ndarray (if present),\n",
    "                'other': np.ndarray (if present),\n",
    "                'mixture': np.ndarray,  # Combined augmented stems\n",
    "                'descriptions': {'vocals': str, 'bass': str, 'drums': str, 'other': str},\n",
    "                'track_name': str  # Name of the track with '_aug' suffix\n",
    "            }\n",
    "    \"\"\"\n",
    "    if idx >= len(mus_train.tracks) -1:\n",
    "        raise ValueError(f\"Index {idx} is out of bounds for the training set with {len(mus_train.tracks)} tracks.\")\n",
    "    \n",
    "    track = extract_track_by_index(mus_train, idx, to_mono=True)\n",
    "    sr = track.rate\n",
    "    print(f\"Selected track: {track.name} (index {idx})\")\n",
    "\n",
    "    # =========================== #\n",
    "    # VALIDATE DURATION PARAMETER #\n",
    "    # =========================== #\n",
    "    if duration is None:\n",
    "        raise ValueError(\"Duration must be specified for coherent augmentation.\")\n",
    "    if duration <= 0:\n",
    "        raise ValueError(\"Duration must be a positive number.\")\n",
    "    if duration > track.audio.shape[0] / sr:\n",
    "        raise ValueError(f\"Duration {duration} seconds exceeds track length {track.audio.shape[0] / sr:.2f} seconds.\")\n",
    "    if duration < 1.0:\n",
    "        raise ValueError(\"Duration must be at least 1 second for coherent augmentation.\")\n",
    "\n",
    "    # =========================== #\n",
    "    # CHECKING STEMS AVAILABILITY #\n",
    "    # =========================== #\n",
    "    stem_names = ['vocals', 'bass', 'drums', 'other']\n",
    "    missing_stems = [stem for stem in stem_names if stem not in track.sources]\n",
    "    if missing_stems:\n",
    "        raise ValueError(f\"Track '{track.name}' is missing the following stems: {', '.join(missing_stems)}\")\n",
    "\n",
    "    # =============================== #\n",
    "    # FIND SUITABLE SEGMENT FOR STEMS #\n",
    "    # =============================== #\n",
    "    \n",
    "    # Calculate maximum possible offset to ensure we can extract the full duration\n",
    "    max_offset = track.audio.shape[0] / sr - duration\n",
    "    \n",
    "    # Initialize variables\n",
    "    all_stems_suitable = False\n",
    "    attempt_count = 0\n",
    "    \n",
    "    while not all_stems_suitable and attempt_count < max_attempts:\n",
    "        attempt_count += 1\n",
    "        print(f\"Attempt {attempt_count}/{max_attempts} to find suitable stems segment...\")\n",
    "        \n",
    "        # Select a random offset time to start processing\n",
    "        offset = random.uniform(0, max_offset)\n",
    "        start_sample = int(offset * sr)\n",
    "        end_sample = start_sample + int(duration * sr)\n",
    "        \n",
    "        # Check if all stems in this segment are suitable\n",
    "        stems_suitability = {}\n",
    "        all_suitable = True\n",
    "        \n",
    "        for stem in stem_names:\n",
    "            # Get the original audio for this stem\n",
    "            original_audio = track.sources[stem].audio[start_sample:end_sample]\n",
    "            \n",
    "            # Check if this stem segment is suitable\n",
    "            stems_suitability[stem] = is_suitable(\n",
    "                original_audio, \n",
    "                energy_threshold=0.005, \n",
    "                silent_threshold=0.002, \n",
    "                min_active_ratio=0.3, \n",
    "                window_size=4410\n",
    "            )\n",
    "            \n",
    "            if not stems_suitability[stem]:\n",
    "                all_suitable = False\n",
    "        \n",
    "        # Print suitability results for this attempt\n",
    "        print(f\"Segment at offset {offset:.2f}s:\")\n",
    "        for stem, suitable in stems_suitability.items():\n",
    "            print(f\"  - {stem}: {'Suitable' if suitable else 'Not suitable'}\")\n",
    "        \n",
    "        if all_suitable:\n",
    "            all_stems_suitable = True\n",
    "            print(f\"Found suitable segment after {attempt_count} attempts at offset {offset:.2f}s\")\n",
    "        elif attempt_count == max_attempts:\n",
    "            print(f\"Warning: Could not find segment with all suitable stems after {max_attempts} attempts.\")\n",
    "            print(\"Proceeding with the last segment checked.\")\n",
    "\n",
    "    # ============================== #\n",
    "    # INITIALIZE RETURN DICTIONARIES #\n",
    "    # ============================== #\n",
    "    original_stems = {}\n",
    "    augmented_stems = {}\n",
    "    augmented_stems['descriptions'] = {}\n",
    "    original_stems['track_name'] = track.name\n",
    "    augmented_stems['track_name'] = track.name + \"_aug\"\n",
    "\n",
    "\n",
    "    # ================================================ #\n",
    "    # DECIDE ON FIXED AUGMENTATION TYPES AND PARAMETERS #\n",
    "    # ================================================ #\n",
    "    fixed_augmentations = []\n",
    "    \n",
    "    # 60% chance to add each augmentation type\n",
    "    if random.random() < 0.6:\n",
    "        fixed_augmentations.append('pitch_shift')\n",
    "    if random.random() < 0.6:\n",
    "        fixed_augmentations.append('time_stretch')\n",
    "    if random.random() < 0.6:\n",
    "        fixed_augmentations.append('compression')\n",
    "    if random.random() < 0.6:\n",
    "        fixed_augmentations.append('reverb')\n",
    "    # if random.random() < 0.6:\n",
    "    #     fixed_augmentations.append('noise')\n",
    "    \n",
    "    # Pre-generate all the random parameters\n",
    "    fixed_params = {}\n",
    "    \n",
    "    if 'pitch_shift' in fixed_augmentations:\n",
    "        fixed_params['pitch_shift'] = random.uniform(*config.PITCH_SHIFT_RANGE)\n",
    "\n",
    "    if 'time_stretch' in fixed_augmentations:\n",
    "        fixed_params['time_stretch'] = random.uniform(*config.TIME_STRETCH_RANGE)\n",
    "    \n",
    "    if 'compression' in fixed_augmentations:\n",
    "        fixed_params['compression_threshold'] = random.uniform(*config.COMPRESSION_THRESHOLD_RANGE)\n",
    "        fixed_params['compression_ratio']     = random.uniform(*config.COMPRESSION_RATIO_RANGE)\n",
    "    \n",
    "    if 'reverb' in fixed_augmentations:\n",
    "        fixed_params['reverb_room_size'] = random.uniform(*config.REVERB_ROOM_SIZE_RANGE)\n",
    "        fixed_params['reverb_damping']   = random.uniform(*config.REVERB_DAMPING_RANGE)\n",
    "    \n",
    "    # if 'noise' in fixed_augmentations:\n",
    "    #     fixed_params['noise_level'] = random.uniform(*config.NOISE_LEVEL_RANGE)\n",
    "    \n",
    "    # ================================= #\n",
    "    # APPLY AUGMENTATIONS FOR ALL STEMS #\n",
    "    # ================================= #\n",
    "    for stem in stem_names:\n",
    "        # Get the original audio for this stem\n",
    "        original_audio = track.sources[stem].audio[start_sample:end_sample]\n",
    "        \n",
    "        # Start with the original audio\n",
    "        augmented_audio = original_audio.copy()\n",
    "        applied_augmentations = []\n",
    "        \n",
    "        # Apply each selected augmentation with fixed parameters\n",
    "        if 'pitch_shift' in fixed_augmentations:\n",
    "            n_steps = fixed_params['pitch_shift']\n",
    "            augmented_audio = pitch_shift_audio(augmented_audio, sr=sr, n_steps=n_steps)\n",
    "            applied_augmentations.append(f\"Pitch shift: {n_steps:.2f} semitones\")\n",
    "        \n",
    "        if 'time_stretch' in fixed_augmentations:\n",
    "            stretch_factor = fixed_params['time_stretch']\n",
    "            augmented_audio = time_stretch_audio(augmented_audio, stretch_factor=stretch_factor)\n",
    "            applied_augmentations.append(f\"Time stretch: {stretch_factor:.2f}x\")\n",
    "        \n",
    "        # if 'noise' in fixed_augmentations:\n",
    "        #     noise_level = fixed_params['noise_level']\n",
    "        #     augmented_audio = add_noise_audio(augmented_audio, noise_level=noise_level)\n",
    "        #     applied_augmentations.append(f\"Noise: {noise_level:.4f} std dev\")\n",
    "        \n",
    "        if 'compression' in fixed_augmentations:\n",
    "            threshold = fixed_params['compression_threshold']\n",
    "            ratio = fixed_params['compression_ratio']\n",
    "            augmented_audio = compress_dynamic_range_audio(augmented_audio, threshold=threshold, ratio=ratio)\n",
    "            applied_augmentations.append(f\"Compression: {ratio:.1f}:1 @ {threshold:.2f}\")\n",
    "        \n",
    "        if 'reverb' in fixed_augmentations:\n",
    "            room_size = fixed_params['reverb_room_size']\n",
    "            damping = fixed_params['reverb_damping']\n",
    "            augmented_audio = apply_reverb_audio(augmented_audio, sr=sr, room_size=room_size, damping=damping)\n",
    "            applied_augmentations.append(f\"Reverb: room={room_size:.2f}, damp={damping:.2f}\")\n",
    "        \n",
    "        # # Normalize to prevent clipping\n",
    "        # max_val = np.max(np.abs(augmented_audio))\n",
    "        # if max_val > 0.95:\n",
    "        #     augmented_audio = augmented_audio / max_val * 0.95\n",
    "        #     applied_augmentations.append(\"Normalized\")\n",
    "        \n",
    "        description = \"; \".join(applied_augmentations) if applied_augmentations else \"No augmentation\"\n",
    "        \n",
    "        # Ensure the augmented audio is of the correct duration\n",
    "        if len(augmented_audio) < int(duration * sr):\n",
    "            # Pad with zeros if shorter than duration\n",
    "            padding_length = int(duration * sr) - len(augmented_audio)\n",
    "            augmented_audio = np.pad(augmented_audio, (0, padding_length), mode='constant')\n",
    "        elif len(augmented_audio) > int(duration * sr):\n",
    "            # Trim to the specified duration if longer\n",
    "            augmented_audio = augmented_audio[:int(duration * sr)]\n",
    "\n",
    "        # Store in return dictionaries\n",
    "        original_stems[stem] = original_audio/ np.max(np.abs(original_audio))  # Normalize original audio to [-1, 1]\n",
    "        augmented_stems[stem] = augmented_audio / np.max(np.abs(augmented_audio))  # Normalize augmented audio to [-1, 1]\n",
    "        augmented_stems['descriptions'][stem] = description\n",
    "\n",
    "        if show_audio:\n",
    "            print(f\"Stem: {stem}\")\n",
    "            display(Audio(original_audio, rate=sr))\n",
    "            print(f\"Augmentation: {description}\")\n",
    "            display(Audio(augmented_audio, rate=sr))\n",
    "            print()\n",
    "\n",
    "    # ====================================================== #\n",
    "    # COMBINE ALL ORIGINAL AND AUGMENTED STEMS INTO 2 ARRAYS # \n",
    "    # ====================================================== #\n",
    "\n",
    "    original_combined  = np.sum([(original_stems[stem]/4) for stem in stem_names if stem in original_stems], axis=0)\n",
    "    augmented_combined = np.sum([(augmented_stems[stem]/4) for stem in stem_names if stem in augmented_stems], axis=0)\n",
    "\n",
    "    # Add both combined mixtures to the results\n",
    "    original_stems['mixture'] = original_combined\n",
    "    augmented_stems['mixture'] = augmented_combined\n",
    "\n",
    "    # =================================== #\n",
    "    # DISPLAY COMBINED AUDIO IF REQUESTED #\n",
    "    # =================================== #\n",
    "    if show_audio:  \n",
    "        print(\"\\nOriginal mixture of all stems:\")\n",
    "        display(Audio(original_combined, rate=sr))\n",
    "\n",
    "        print(\"\\nAugmented mixture of all stems:\")\n",
    "        display(Audio(augmented_combined, rate=sr))\n",
    "\n",
    "    print(\"Augmentation complete\")\n",
    "    return original_stems, augmented_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_stems_fixed, aug_stems_fixed = coherent_augmentation_fixed_params(mus_train, idx=1, config=config, duration=10.0, show_audio=True)\n",
    "print(f\"Original Track name: {orig_stems_fixed['track_name']}\")\n",
    "print(f\"Augmented Track name: {aug_stems_fixed['track_name']}\\n\")\n",
    "\n",
    "print(\"Original stems:\")\n",
    "for stem, audio in orig_stems_fixed.items():\n",
    "    if stem != 'track_name':\n",
    "        print(f\"{stem.capitalize()}: {audio.shape} samples\")\n",
    "\n",
    "print(\"Augmented stems:\")\n",
    "for stem, audio in aug_stems_fixed.items():\n",
    "    if stem != 'mixture' and stem != 'descriptions' and stem != 'track_name':\n",
    "        print(f\"{stem.capitalize()}: {audio.shape} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stem in aug_stems_fixed.keys():\n",
    "    if stem != 'descriptions' and stem != 'track_name':\n",
    "        plot_audio_comparison_with_melspec(\n",
    "            orig_stems_fixed[stem], \n",
    "            aug_stems_fixed[stem], \n",
    "            sample_rate=44100, \n",
    "            title=f\"Original vs Augmented {stem.capitalize()} Audio\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16305a98",
   "metadata": {},
   "source": [
    "### Incoherent Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incoherent_augmentation(mus_train, config=config, duration=10.0, show_audio=False, max_attempts=10):\n",
    "    \"\"\"\n",
    "    Select four random stems from FOUR DIFFERENT tracks in the MUSDB18 training set,\n",
    "    apply random augmentations to each stem, and combine them into a mixture.\n",
    "    Ensures all stems are suitable (have enough energy and non-silent segments) before processing.\n",
    "    \n",
    "    Args:\n",
    "        mus_train (musdb.DB): MUSDB18 training set object\n",
    "        config: Configuration object for augmentation parameters\n",
    "        duration (float): Duration in seconds to process for each stem\n",
    "        show_audio (bool): If True, display audio players for original and augmented stems\n",
    "        max_attempts (int): Maximum number of attempts to find suitable stem segments per track\n",
    "\n",
    "    Returns:\n",
    "        tuple: (original_stems, augmented_stems)\n",
    "            original_stems: {\n",
    "                'vocals': np.ndarray (if present),\n",
    "                'bass': np.ndarray (if present),\n",
    "                'drums': np.ndarray (if present),\n",
    "                'other': np.ndarray (if present),\n",
    "                'mixture': np.ndarray,  # Combined original stems\n",
    "                'track_name': INCOHERENT_TRACK_NAME + random integer  # Name of the track\n",
    "            }\n",
    "            augmented_stems: {\n",
    "                'vocals': np.ndarray (if present),\n",
    "                'bass': np.ndarray (if present),\n",
    "                'drums': np.ndarray (if present),\n",
    "                'other': np.ndarray (if present),\n",
    "                'mixture': np.ndarray,  # Combined augmented stems\n",
    "                'descriptions': {'vocals': str, 'bass': str, 'drums': str, 'other': str},\n",
    "                'track_name': {stem: track_name + \"_aug\"}  # Names of the tracks for each stem with '_aug' suffix\n",
    "            }\n",
    "    \"\"\"\n",
    "    # Standard stem names in MUSDB18\n",
    "    stem_names = ['vocals', 'bass', 'drums', 'other']\n",
    "    \n",
    "    # =========================== #\n",
    "    # VALIDATE DURATION PARAMETER #\n",
    "    # =========================== #\n",
    "    if duration is None:\n",
    "        raise ValueError(\"Duration must be specified for incoherent augmentation.\")\n",
    "    if duration <= 0:\n",
    "        raise ValueError(\"Duration must be a positive number.\")\n",
    "    if duration < 1.0:\n",
    "        raise ValueError(\"Duration must be at least 1 second for incoherent augmentation.\")\n",
    "    \n",
    "    # Initialize return dictionaries\n",
    "    original_stems = {}\n",
    "    augmented_stems = {}\n",
    "    augmented_stems['descriptions'] = {}\n",
    "    original_stems['track_name'] = \"INCOHERENT_TRACK_NAME_\" + str(random.randint(1, 9999))\n",
    "    augmented_stems['track_name'] = \"AUG_\" + original_stems['track_name']\n",
    "    \n",
    "    # For each stem type, select a random track and extract the stem\n",
    "    for stem in stem_names:\n",
    "        stem_found = False\n",
    "        attempts = 0\n",
    "        \n",
    "        while not stem_found and attempts < max_attempts * 3:  # More attempts since we need to match 4 different tracks\n",
    "            attempts += 1\n",
    "            \n",
    "            # Select a random track\n",
    "            track_idx = random.randint(0, len(mus_train.tracks) - 1)\n",
    "            track = extract_track_by_index(mus_train, track_idx, to_mono=True)\n",
    "            sr = track.rate\n",
    "            \n",
    "            # Check if the track has this stem\n",
    "            if stem not in track.sources:\n",
    "                raise ValueError(f\"Track '{track.name}' is missing the '{stem}' stem.\")\n",
    "                \n",
    "            # Check if the track is long enough\n",
    "            if track.audio.shape[0] / sr < duration:\n",
    "                raise ValueError(f\"Track '{track.name}' is too short for the specified duration of {duration} seconds.\")\n",
    "            \n",
    "            # Calculate maximum possible offset for this track\n",
    "            max_offset = track.audio.shape[0] / sr - duration\n",
    "            \n",
    "            # Try different random offsets to find a suitable segment\n",
    "            stem_segment_attempts = 0\n",
    "            while stem_segment_attempts < max_attempts:\n",
    "                stem_segment_attempts += 1\n",
    "                \n",
    "                # Select a random offset\n",
    "                offset = random.uniform(0, max_offset)\n",
    "                start_sample = int(offset * sr)\n",
    "                end_sample = start_sample + int(duration * sr)\n",
    "                \n",
    "                # Get the stem audio at this offset\n",
    "                stem_audio = track.sources[stem].audio[start_sample:end_sample]\n",
    "                \n",
    "                # Check if this stem segment is suitable\n",
    "                if is_suitable(stem_audio, \n",
    "                              energy_threshold=0.005, \n",
    "                              silent_threshold=0.002, \n",
    "                              min_active_ratio=0.3, \n",
    "                              window_size=4410):\n",
    "                    # We found a suitable segment for this stem\n",
    "                    print(f\"Found suitable {stem} from track '{track.name}' at offset {offset:.2f}s\")\n",
    "                    original_stems[stem] = stem_audio\n",
    "                    stem_found = True\n",
    "                    break\n",
    "            \n",
    "            if stem_found:\n",
    "                break\n",
    "        \n",
    "        if not stem_found:\n",
    "            raise ValueError(f\"Could not find a suitable {stem} segment after {attempts} attempts across different tracks.\")\n",
    "        \n",
    "        # Create a random list of augmentations to apply\n",
    "        random_augmentations = []\n",
    "        \n",
    "        # 60% chance to add each augmentation type\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('pitch_shift')\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('time_stretch')\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('compression')\n",
    "        if random.random() < 0.6:\n",
    "            random_augmentations.append('reverb')\n",
    "        \n",
    "        # Apply the augmentations\n",
    "        augmented_audio, description = apply_augmentations(\n",
    "            audio=original_stems[stem],\n",
    "            sr=sr,\n",
    "            augmentation_types=random_augmentations,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        # Ensure the augmented audio is the correct duration\n",
    "        if len(augmented_audio) < int(duration * sr):\n",
    "            # Pad with zeros if shorter than duration\n",
    "            padding_length = int(duration * sr) - len(augmented_audio)\n",
    "            augmented_audio = np.pad(augmented_audio, (0, padding_length), mode='constant')\n",
    "        elif len(augmented_audio) > int(duration * sr):\n",
    "            # Trim to the specified duration if longer\n",
    "            augmented_audio = augmented_audio[:int(duration * sr)]\n",
    "        \n",
    "        # Store in return dictionaries\n",
    "        augmented_stems[stem] = augmented_audio\n",
    "        augmented_stems['descriptions'][stem] = description\n",
    "        \n",
    "        if show_audio:\n",
    "            print(f\"Stem: {stem} from track '{track.name}'\")\n",
    "            display(Audio(original_stems[stem], rate=sr))\n",
    "            print(f\"Augmentation: {description}\")\n",
    "            display(Audio(augmented_audio, rate=sr))\n",
    "            print()\n",
    "    \n",
    "    # ====================================================== #\n",
    "    # COMBINE ALL ORIGINAL AND AUGMENTED STEMS INTO 2 ARRAYS # \n",
    "    # ====================================================== #\n",
    "    \n",
    "    original_combined = np.sum([(original_stems[stem]/4) for stem in stem_names], axis=0) \n",
    "    augmented_combined = np.sum([(augmented_stems[stem]/4) for stem in stem_names], axis=0)\n",
    "\n",
    "\n",
    "    # Add both combined mixtures to the results\n",
    "    original_stems['mixture'] = original_combined\n",
    "    augmented_stems['mixture'] = augmented_combined\n",
    "    \n",
    "    # =================================== #\n",
    "    # DISPLAY COMBINED AUDIO IF REQUESTED #\n",
    "    # =================================== #\n",
    "    if show_audio:  \n",
    "        print(\"\\nOriginal mixture of stems from different tracks:\")\n",
    "        display(Audio(original_combined, rate=sr))\n",
    "        \n",
    "        print(\"\\nAugmented mixture of stems from different tracks:\")\n",
    "        display(Audio(augmented_combined, rate=sr))\n",
    "    \n",
    "    print(\"Incoherent augmentation complete\")\n",
    "    return original_stems, augmented_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7780f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "incoherent_orig_stems, incoherent_aug_stems = incoherent_augmentation(mus_train, config=config, duration=10.0, show_audio=True)\n",
    "\n",
    "print(f\"Original Track name: {incoherent_orig_stems['track_name']}\")\n",
    "print(f\"Augmented Track name: {incoherent_aug_stems['track_name']}\\n\")\n",
    "\n",
    "print(\"Original stems from incoherent augmentation:\")\n",
    "for stem, audio in incoherent_orig_stems.items():\n",
    "    if stem != 'mixture' and stem != 'track_name':\n",
    "        print(f\"{stem.capitalize()}: {audio.shape} samples\")\n",
    "print(\"Augmented stems from incoherent augmentation:\")\n",
    "for stem, audio in incoherent_aug_stems.items():\n",
    "    if stem != 'mixture' and stem != 'descriptions' and stem != 'track_name':\n",
    "        print(f\"{stem.capitalize()}: {audio.shape} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a666cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stem in incoherent_aug_stems.keys():\n",
    "    if stem != 'descriptions' and stem != 'track_name':\n",
    "        plot_audio_comparison_with_melspec(\n",
    "            incoherent_orig_stems[stem], \n",
    "            incoherent_aug_stems[stem], \n",
    "            sample_rate=44100, \n",
    "            title=f\"Original vs Augmented {stem.capitalize()} Audio (Incoherent)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incoherent_orig_stems.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac83470",
   "metadata": {},
   "source": [
    "### Save augmented stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_stems(augmented_stems, output_dir):\n",
    "    \"\"\"\n",
    "    Save the augmented stems to the specified output directory.\n",
    "    \n",
    "    Args:\n",
    "        augmented_stems (dict): Dictionary containing augmented stems and their descriptions\n",
    "        output_dir (str): Directory where the stems will be saved\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'mixture'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'vocals'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'bass'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'drums'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'other'), exist_ok=True)\n",
    "\n",
    "    aug_track_name = augmented_stems['track_name']\n",
    "\n",
    "    for stem, audio in augmented_stems.items():\n",
    "        if stem not in ['descriptions', 'track_name']:\n",
    "            file_path = os.path.join(output_dir, stem, f\"{aug_track_name}_{stem}.wav\")\n",
    "            sf.write(file_path, audio, 44100)\n",
    "            print(f\"Saved {stem} stem to {file_path}\")\n",
    "    \n",
    "    print(f\"All augmented stems for '{aug_track_name}' saved to {output_dir}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/Users/agathe/Desktop/Coherent_Augmentation\"   # Output directory for augmented files\n",
    "save_augmented_stems(incoherent_aug_stems, OUTPUT_DIR)\n",
    "print(\"\\n\")\n",
    "save_augmented_stems(aug_stems_fixed, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbecd23",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc0fa5",
   "metadata": {},
   "source": [
    "### Coherent Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(mus_train.tracks), 40): # Process every 40th track for demonstration\n",
    "    print(f\"\\nProcessing track at index {idx}...\")\n",
    "    try:\n",
    "        orig_stems, aug_stems = coherent_augmentation_fixed_params(mus_train, idx=idx, config=config, duration=10.0, show_audio=False)\n",
    "        save_augmented_stems(aug_stems, config.OUTPUT_DIR_COHERENT_MIX)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing track at index {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b56140",
   "metadata": {},
   "source": [
    "### Incoherent Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(mus_train.tracks), 40):  # Process every 40th track for demonstration\n",
    "    print(f\"\\nProcessing track at index {idx}...\")\n",
    "    try:\n",
    "        orig_stems, aug_stems = incoherent_augmentation(mus_train, config=config, duration=10.0, show_audio=False)\n",
    "        save_augmented_stems(aug_stems, config.OUTPUT_DIR_INCOHERENT_MIX)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing track at index {idx}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STMAE_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
